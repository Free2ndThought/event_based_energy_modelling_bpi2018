{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook shows the generation methodology of the bpmi challenge energy data generation.\n",
    "\n",
    "First a simple pandas dataframe is created, symbolizing one machine that is operating every process"
   ],
   "id": "5c219f706dd86850"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T12:11:23.966830Z",
     "start_time": "2025-04-21T12:11:23.575858Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from six import iteritems\n",
    "\n",
    "# From a start date, end date, and frequency\n",
    "start = '2018-01-01'\n",
    "end = '2018-12-31'\n",
    "dti = pd.date_range(start, end, freq='15min')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add to each data point a periodic signal based on two factors; the time of day based on a broadened sine wave, which peaks after lunch hours and reaches the minimum 12h later. The amplitude is modified based on the workday, so it is diminished by 60% on weekends.",
   "id": "f1357f31c407c766"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T12:11:25.212095Z",
     "start_time": "2025-04-21T12:11:24.169041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_sine_wave(timestamps, amplitude=1.0, vertical_shift=0.0, peak_hour_shift=14.0):\n",
    "    \"\"\"\n",
    "    Generate a sine wave signal based on timestamps with a 24-hour period.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    timestamps : list-like or pandas Series\n",
    "        Timestamps in format 'YYYY:MM:DD:hh:mm:ss'\n",
    "    amplitude : float, optional (default=1.0)\n",
    "        The amplitude of the sine wave\n",
    "    vertical_shift : float, optional (default=0.0)\n",
    "        Vertical shift of the entire wave (moves the wave up or down)\n",
    "    peak_hour_shift : float, optional (default=14.0)\n",
    "        Hour of the day when the peak should occur (default 14.0 = 2 PM)\n",
    "        Can be fractional for fine-tuning\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series : Sine wave values corresponding to input timestamps\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert timestamps to datetime objects\n",
    "    if isinstance(timestamps, str):\n",
    "        timestamps = [timestamps]\n",
    "\n",
    "    datetime_objects = pd.Series([\n",
    "        datetime.strptime(ts, '%Y:%m:%d:%H:%M:%S')\n",
    "        for ts in timestamps\n",
    "    ])\n",
    "\n",
    "    # Extract hours and minutes as fractional hours\n",
    "    hours = datetime_objects.dt.hour + datetime_objects.dt.minute/60 + datetime_objects.dt.second/3600\n",
    "\n",
    "    # Calculate phase shift based on peak_hour_shift\n",
    "    # We want the peak at peak_hour_shift hours\n",
    "    phase_shift = (peak_hour_shift - 6) * (2 * np.pi / 24)  # -6 because sine peaks at π/2\n",
    "\n",
    "    # Generate sine wave\n",
    "    # Period is 24 hours = 2π\n",
    "    angular_freq = 2 * np.pi / 24\n",
    "    sine_wave = amplitude * np.sin(angular_freq * hours - phase_shift) + vertical_shift\n",
    "    sine_wave_series = pd.Series(sine_wave)\n",
    "    sine_wave_series.index = datetime_objects\n",
    "    return sine_wave_series\n",
    "\n",
    "def generate_timestamp_range(start_timestamp, end_timestamp, freq='15min'):\n",
    "    \"\"\"\n",
    "    Generate a range of timestamps between start and end times.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_timestamp : str\n",
    "        Start timestamp in format 'YYYY:MM:DD:hh:mm:ss'\n",
    "    end_timestamp : str\n",
    "        End timestamp in format 'YYYY:MM:DD:hh:mm:ss'\n",
    "    freq : str, optional (default='1H')\n",
    "        Frequency of timestamps (pandas frequency string)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list : List of timestamp strings in the specified format\n",
    "    \"\"\"\n",
    "    start_dt = datetime.strptime(start_timestamp, '%Y:%m:%d:%H:%M:%S')\n",
    "    end_dt = datetime.strptime(end_timestamp, '%Y:%m:%d:%H:%M:%S')\n",
    "\n",
    "    date_range = pd.date_range(start=start_dt, end=end_dt, freq=freq)\n",
    "    return [dt.strftime('%Y:%m:%d:%H:%M:%S') for dt in date_range]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate timestamps for 3 days with hourly intervals\n",
    "    timestamps = generate_timestamp_range(\n",
    "        '2018:01:01:00:00:00',\n",
    "        '2018:12:31:23:59:59',\n",
    "        freq='15min'\n",
    "    )\n",
    "\n",
    "    # Generate different sine waves with various parameters\n",
    "\n",
    "    normal_wave = generate_sine_wave(timestamps)\n",
    "    shifted_up = generate_sine_wave(timestamps, vertical_shift=2.0)\n",
    "    larger_amplitude = generate_sine_wave(timestamps, amplitude=2.0)\n",
    "    peak_at_15 = generate_sine_wave(timestamps, peak_hour_shift=15.0)  # Peak at 3 PM\n",
    "\n",
    "    # Create DataFrame with all waves\n",
    "    results = pd.DataFrame({\n",
    "        'normal': normal_wave,\n",
    "        'shifted_up': shifted_up,\n",
    "        'larger_amplitude': larger_amplitude,\n",
    "        'peak_at_15': peak_at_15,\n",
    "    })\n",
    "    results.index = timestamps\n",
    "\n",
    "    print(results.head(24))"
   ],
   "id": "26f6f7414d10caa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       normal  shifted_up  larger_amplitude  peak_at_15\n",
      "2018:01:01:00:00:00 -0.866025    1.133975         -1.732051   -0.707107\n",
      "2018:01:01:00:15:00 -0.896873    1.103127         -1.793745   -0.751840\n",
      "2018:01:01:00:30:00 -0.923880    1.076120         -1.847759   -0.793353\n",
      "2018:01:01:00:45:00 -0.946930    1.053070         -1.893860   -0.831470\n",
      "2018:01:01:01:00:00 -0.965926    1.034074         -1.931852   -0.866025\n",
      "2018:01:01:01:15:00 -0.980785    1.019215         -1.961571   -0.896873\n",
      "2018:01:01:01:30:00 -0.991445    1.008555         -1.982890   -0.923880\n",
      "2018:01:01:01:45:00 -0.997859    1.002141         -1.995718   -0.946930\n",
      "2018:01:01:02:00:00 -1.000000    1.000000         -2.000000   -0.965926\n",
      "2018:01:01:02:15:00 -0.997859    1.002141         -1.995718   -0.980785\n",
      "2018:01:01:02:30:00 -0.991445    1.008555         -1.982890   -0.991445\n",
      "2018:01:01:02:45:00 -0.980785    1.019215         -1.961571   -0.997859\n",
      "2018:01:01:03:00:00 -0.965926    1.034074         -1.931852   -1.000000\n",
      "2018:01:01:03:15:00 -0.946930    1.053070         -1.893860   -0.997859\n",
      "2018:01:01:03:30:00 -0.923880    1.076120         -1.847759   -0.991445\n",
      "2018:01:01:03:45:00 -0.896873    1.103127         -1.793745   -0.980785\n",
      "2018:01:01:04:00:00 -0.866025    1.133975         -1.732051   -0.965926\n",
      "2018:01:01:04:15:00 -0.831470    1.168530         -1.662939   -0.946930\n",
      "2018:01:01:04:30:00 -0.793353    1.206647         -1.586707   -0.923880\n",
      "2018:01:01:04:45:00 -0.751840    1.248160         -1.503680   -0.896873\n",
      "2018:01:01:05:00:00 -0.707107    1.292893         -1.414214   -0.866025\n",
      "2018:01:01:05:15:00 -0.659346    1.340654         -1.318692   -0.831470\n",
      "2018:01:01:05:30:00 -0.608761    1.391239         -1.217523   -0.793353\n",
      "2018:01:01:05:45:00 -0.555570    1.444430         -1.111140   -0.751840\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On top of the periodic signal, the energy generator receives a significant amount of noise in order to account for energy consumption unrelated to the economic processes of the machines like OS calculations, overhead storage, appliances that are not related to customer processes, updates or similar occurences.",
   "id": "39c3194efc9578f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T12:11:25.234481Z",
     "start_time": "2025-04-21T12:11:25.230503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def noise_gen(noise_scaling=1):\n",
    "    while True:\n",
    "        yield noise_scaling * random.uniform(80,90)\n",
    "\n",
    "def apply_noise(value, noise):\n",
    "    \"\"\"\n",
    "    Applies noise to a given value.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    value : float\n",
    "        The original value to which noise will be applied\n",
    "    noise : float\n",
    "        The noise factor to apply\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float : The value after applying noise\n",
    "    \"\"\"\n",
    "    return value * noise"
   ],
   "id": "200e37ae9dbee110",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The relevant component of the signal generation is the actual amount of energy that is typically consumed by the specific processes that are running in the current time interval. In this step the number of running processes is computed using the event log and for each process event the postulated energy consumption is added in accordance with the event type. Additionally, a small amount of surplus energy is added whenever an event has been processed but is still running, or some events are surpassed by follow up events. This is done in order to account for reading, saving and communication processing that occurs whenever states have to change or previous steps have to be saved for later processing.",
   "id": "6ab68cb6751c369c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T12:11:26.530783Z",
     "start_time": "2025-04-21T12:11:25.248132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from math import floor\n",
    "import event_object\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "def distribute_event_costs_series(energy_series: pd.Series,\n",
    "                           events: List['EventObject'],\n",
    "                           cost_mapping: Dict[str, float],\n",
    "                           timestamp_format: str = '%Y:%m:%d:%H:%M:%S') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Distributes the cost of events across energy measurement intervals based on event duration.\n",
    "    Works with a pandas Series with timestamps as index and energy values as data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    energy_series : pd.Series\n",
    "        Series with timestamps as index and energy measurements as values\n",
    "    events : List[EventObject]\n",
    "        List of EventObject instances with timestamp and duration attributes\n",
    "    cost_mapping : Dict[str, float]\n",
    "        Dictionary mapping event types to their base costs\n",
    "    timestamp_format : str, default='%Y:%m:%d:%H:%M:%S'\n",
    "        Format string for parsing timestamps if they are strings\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns for timestamp, original energy values, event costs,\n",
    "        and total (energy + cost)\n",
    "    \"\"\"\n",
    "    # Make a copy of the input series to avoid modifying the original\n",
    "    series_copy = energy_series.copy()\n",
    "\n",
    "    # Convert index to datetime if it isn't already\n",
    "    if not isinstance(series_copy.index, pd.DatetimeIndex):\n",
    "        # If index is not already datetime, attempt to convert\n",
    "        try:\n",
    "            # Try to convert using the specified format\n",
    "            timestamps = pd.to_datetime(series_copy.index, format=timestamp_format)\n",
    "            series_copy.index = timestamps\n",
    "        except ValueError:\n",
    "            # Fall back to pandas default parser\n",
    "            try:\n",
    "                timestamps = pd.to_datetime(series_copy.index)\n",
    "                series_copy.index = timestamps\n",
    "            except:\n",
    "                raise ValueError(\n",
    "                    f\"Could not parse timestamps with format '{timestamp_format}'. \"\n",
    "                    \"Please check your data or provide the correct format.\"\n",
    "                )\n",
    "\n",
    "    # Create DataFrame from Series, handling unnamed index\n",
    "    if series_copy.index.name:\n",
    "        # Use the existing index name\n",
    "        timestamp_col_name = series_copy.index.name\n",
    "    else:\n",
    "        # Use a default name if index has no name\n",
    "        timestamp_col_name = 'timestamp'\n",
    "\n",
    "    # Reset index to convert index to column and name it appropriately\n",
    "    result_df = series_copy.reset_index()\n",
    "\n",
    "    # Rename the index column if it's unnamed\n",
    "    if result_df.columns[0] == 'index':\n",
    "        result_df.rename(columns={'index': timestamp_col_name}, inplace=True)\n",
    "\n",
    "    # Rename the values column to 'energy'\n",
    "    if series_copy.name:\n",
    "        energy_col_name = series_copy.name\n",
    "    else:\n",
    "        energy_col_name = 'energy'\n",
    "        result_df.rename(columns={0: energy_col_name}, inplace=True)\n",
    "\n",
    "    # Ensure the timestamp column is in datetime format\n",
    "    timestamp_col = result_df.columns[0]  # First column contains timestamps\n",
    "    if not pd.api.types.is_datetime64_any_dtype(result_df[timestamp_col]):\n",
    "        result_df[timestamp_col] = pd.to_datetime(result_df[timestamp_col])\n",
    "\n",
    "    # Sort the dataframe by timestamp\n",
    "    result_df = result_df.sort_values(by=timestamp_col)\n",
    "\n",
    "    # Add a new column for event costs, initialized to 0\n",
    "    result_df['event_cost'] = 0.0\n",
    "\n",
    "    # Calculate the interval duration (assuming equal intervals)\n",
    "    if len(result_df) > 1:\n",
    "        interval_duration = (result_df[timestamp_col].iloc[1] -\n",
    "                            result_df[timestamp_col].iloc[0]).total_seconds()\n",
    "    else:\n",
    "        raise ValueError(\"Energy Series must have at least two elements to determine interval duration\")\n",
    "\n",
    "    # copy result_df three times for each basecost_scaling\n",
    "    result_df_list = []\n",
    "    result_df_list.append(result_df)\n",
    "    result_df_list.append(result_df.copy())\n",
    "    result_df_list.append(result_df.copy())\n",
    "    # Process each event\n",
    "    for event_type in events:\n",
    "        # Check if the event type has a defined cost\n",
    "        processed_types = 0\n",
    "        print(f\"Processing {event_type}: {processed_types/len(events)*100}% processed\")\n",
    "        if event_type not in cost_mapping:\n",
    "            continue\n",
    "        print(f\"{len(events[event_type])} to process for {event_type}\")\n",
    "        base_cost = cost_mapping[event_type]\n",
    "        for event in events[event_type]:\n",
    "            # Ensure event timestamp is in datetime format\n",
    "            event_start = event.timestamp\n",
    "            if isinstance(event_start, str):\n",
    "                try:\n",
    "                    event_start = datetime.strptime(event_start, timestamp_format)\n",
    "                except ValueError:\n",
    "                    # Try default parsing if format doesn't match\n",
    "                    event_start = pd.to_datetime(event_start)\n",
    "\n",
    "            event_start = event_start\n",
    "            event_end = event_start + pd.Timedelta(seconds=event.duration)\n",
    "                    # Ensure event timestamps are timezone-naive\n",
    "            if event_start.tz is not None:\n",
    "                event_start = event_start.tz_localize(None)\n",
    "            if event_end.tz is not None:\n",
    "                event_end = event_end.tz_localize(None)\n",
    "\n",
    "\n",
    "        # Find all intervals that overlap with the event\n",
    "\n",
    "            interval_ends = pd.Series(\n",
    "                index=result_df.index,\n",
    "                data=list(result_df.index[1:]) + [result_df.index[-1] + floor(event.duration/pd.Timedelta(seconds=interval_duration).total_seconds())\n",
    "    ])\n",
    "\n",
    "            #mask = (result_df[timestamp_col] <= event_end) & (interval_ends >= event_start)\n",
    "\n",
    "            overlapping_intervals = result_df[\n",
    "                (result_df[timestamp_col] <= event_end.to_datetime64()) &\n",
    "                (result_df[timestamp_col].shift(-1, fill_value=result_df[timestamp_col].iloc[-1] +\n",
    "                                         timedelta(seconds=interval_duration)) >= event_start.to_datetime64())\n",
    "            ]\n",
    "\n",
    "            if overlapping_intervals.empty:\n",
    "                continue\n",
    "\n",
    "        # Calculate the overlap duration and distribute cost for each interval\n",
    "            for idx, row in overlapping_intervals.iterrows():\n",
    "                interval_start = row[timestamp_col]\n",
    "\n",
    "                # Calculate interval end (using next row's timestamp or adding interval duration for the last row)\n",
    "                if idx < result_df.index.max():\n",
    "                    next_idx = result_df.index[result_df.index.get_loc(idx) + 1]\n",
    "                    interval_end = result_df.loc[next_idx, timestamp_col]\n",
    "                else:\n",
    "                    interval_end = interval_start + timedelta(seconds=interval_duration)\n",
    "\n",
    "                # Calculate overlap between event and this interval\n",
    "                overlap_start = max(event_start, interval_start)\n",
    "                overlap_end = min(event_end, interval_end)\n",
    "                overlap_duration = (overlap_end - overlap_start).total_seconds()\n",
    "\n",
    "                # Skip if there's no overlap\n",
    "                if overlap_duration <= 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate the proportion of event duration in this interval\n",
    "                proportion = overlap_duration / event.duration\n",
    "\n",
    "                # Distribute cost proportionally\n",
    "                cost_in_interval = base_cost * proportion\n",
    "\n",
    "                # Add to the event_cost column\n",
    "                base_cost_scalings = [0.1, 1, 10]\n",
    "                for result_df, base_cost_scaling in zip(result_df_list, base_cost_scalings):\n",
    "                    result_df.at[idx, 'event_cost'] += cost_in_interval * base_cost_scaling\n",
    "\n",
    "        # Update the processed types count\n",
    "        processed_types += 1\n",
    "\n",
    "    # Add a column with the total (energy + event cost)\n",
    "    for dataframe in result_df_list:\n",
    "        dataframe['total'] = dataframe[energy_col_name] + dataframe['event_cost']\n",
    "\n",
    "    return result_df_list\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Create sample energy dataframe\n",
    "timestamps = pd.date_range(start='2023-01-01', periods=24, freq='1H')\n",
    "energy_df = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'energy': np.random.uniform(10, 20, size=24)  # Random energy values\n",
    "})\n",
    "\n",
    "# Example cost mapping\n",
    "cost_mapping = {\n",
    "    'login': 5.0,\n",
    "    'logout': 2.0,\n",
    "    'purchase': 10.0,\n",
    "    'search': 3.0\n",
    "}\n",
    "\n",
    "# Assume 'events' is your list of EventObject instances\n",
    "\n",
    "# Calculate and add event costs\n",
    "result = distribute_event_costs(energy_df, events, cost_mapping)\n",
    "print(result)\n",
    "\"\"\"\n"
   ],
   "id": "469b27661d367d1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create sample energy dataframe\\ntimestamps = pd.date_range(start='2023-01-01', periods=24, freq='1H')\\nenergy_df = pd.DataFrame({\\n    'timestamp': timestamps,\\n    'energy': np.random.uniform(10, 20, size=24)  # Random energy values\\n})\\n\\n# Example cost mapping\\ncost_mapping = {\\n    'login': 5.0,\\n    'logout': 2.0,\\n    'purchase': 10.0,\\n    'search': 3.0\\n}\\n\\n# Assume 'events' is your list of EventObject instances\\n\\n# Calculate and add event costs\\nresult = distribute_event_costs(energy_df, events, cost_mapping)\\nprint(result)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With all elements present the choices of the energy generator can be made. The energy generator will take the following parameters:\n",
    "- **01.01.2018**: The start date of the energy generation period.\n",
    "- **31.12.2018**: The end date of the energy generation period.\n",
    "- **1**: The number of devices to be simulated.\n",
    "- **1**: The vertical shift of the sine wave.\n",
    "\n",
    "In order to gauge the efficacy of the energy evaluation the following parameters will be differed in each run:\n",
    "- **0.1 - 10**: The scaling factor for the noise generator.\n",
    "- **0.1 - 10**: The scaling factor for the event base costs.\n",
    "- **0.5 - 1.5**: The amplitude of the sine wave.\n",
    "- **15min - 1h**: The event cost sampling rate.\n",
    "\n",
    "The values are scaled by a factor each repetition to encapsule a wide range of possible values. The concrete values are listed within the parameter lists. Whenever one parameter is changed, the others are fixed at the median value. As each process and implementation differ widely, this way the research will help to identify if an application of the method can be considered."
   ],
   "id": "d897f393b4165c47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T16:39:34.105241Z",
     "start_time": "2025-04-21T12:11:26.549024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "from event_object import EventObject\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#data\n",
    "events = event_object.read_event_objects_from_pickle(\"event_objects_all_variants.pkl\")\n",
    "# fixed parameters\n",
    "start = '2018:01:01:00:00:00'\n",
    "end = '2018:12:31:23:59:59'\n",
    "vertical_shift = 1.1\n",
    "measurements = 1\n",
    "\n",
    "#experiment parameters\n",
    "noise_scaling_list = [0.1, 1, 10]\n",
    "event_base_cost_scaling_list = [0.1, 1, 10]\n",
    "amplitude_scaling_list = [0.5, 1, 1.5]\n",
    "sampling_rate_list = ['15m', '30min', '1h']\n",
    "\n",
    "#Basecosts dictionary\n",
    "__BASECOSTS__ = {\n",
    "    \"Record Goods Receipt\": 99,\n",
    "    \"Create Purchase Order Item\": 98,\n",
    "    \"Record Invoice Receipt\": 97,\n",
    "    \"Vendor creates invoice\": 54,\n",
    "    \"Clear invoice\": 56,\n",
    "    \"Record Service Entry Sheet\": 81,\n",
    "    \"Remove Payment Block\": 59,\n",
    "    \"Create Purchase Requisition Item\": 11,\n",
    "    \"Receive Order Confirmation\": 66,\n",
    "    \"Change Quantity\": 2,\n",
    "    \"Change Price\": 1,\n",
    "    \"Delete Purchase Order item\": 8,\n",
    "    \"Cancel Invoice Receipt\": 9,\n",
    "    \"Change Approval for Purchase Order\": 21,\n",
    "    \"Vendor creates debit memo\": 20,\n",
    "    \"Change Delivery Indicator\": 52,\n",
    "    \"Cancel Goods Receipt\": 10,\n",
    "    \"Release Purchase Order\": 60,\n",
    "    \"SRM: In Transfer to Execution Syst.\": 87,\n",
    "    \"SRM: Created\": 78,\n",
    "    \"SRM: Complete\": 77,\n",
    "    \"SRM: Awaiting Approval\": 63,\n",
    "    \"SRM Document Completed\": 70,\n",
    "    \"SRM: Ordered\": 44,\n",
    "    \"SRM Change was Transmitted\": 92,\n",
    "    \"Reactivate Purchase Order Item\": 57,\n",
    "    \"Block Purchase Order Item\": 31,\n",
    "    \"Cancel Subsequent Invoice\": 32,\n",
    "    \"Change Storage Location\": 36,\n",
    "    \"Update Order Confirmation\": 22,\n",
    "    \"Record Subsequent Invoice\": 40,\n",
    "    \"Release Purchase Requisition\": 62,\n",
    "    \"Set Payment Block\": 26,\n",
    "    \"SRM: Deleted\": 5,\n",
    "    \"Change Currency\": 150,\n",
    "    \"Change Final Invoice Indicator\": 144,\n",
    "    \"SRM: Transaction Completed\": 121,\n",
    "    \"SRM: Incomplete\": 111,\n",
    "    \"SRM: Held\": 88,\n",
    "    \"Change payment term\": 4,\n",
    "    \"Change Rejection Indicator\": 200\n",
    "}\n",
    "\n",
    "def gen_experiment_parameters():\n",
    "    '''\n",
    "    Generates the parameters for the experiment. The parameters are:\n",
    "    - noise_scaling\n",
    "    - event_base_cost_scaling\n",
    "    - amplitude_scaling\n",
    "    - sampling_rate\n",
    "    The parameters are generated in a way that each parameter is fixed at the median value of the other parameters, while changing only one parameter through the list.\n",
    "    :return:\n",
    "    A list of parameters containing [noise_scaling, event_base_cost_scaling, amplitude_scaling, sampling_rate]\n",
    "    '''\n",
    "    for noise in noise_scaling_list:\n",
    "        yield [noise, 1, '30min']\n",
    "\n",
    "    for amplitude_scaling in amplitude_scaling_list:\n",
    "        yield [1, amplitude_scaling, '30min']\n",
    "\n",
    "    for sampling_rate in sampling_rate_list:\n",
    "        yield [1, 1, sampling_rate]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "#dataframe\n",
    "def run_experiment(start, end, vertical_shift,\n",
    "                   noise_scaling,\n",
    "                   amplitude_scaling, sampling_rate):\n",
    "    \"\"\"\n",
    "    Run the experiment with the given parameters.\n",
    "    \"\"\"\n",
    "    # Generate timestamps\n",
    "    dti = generate_timestamp_range(start, end, sampling_rate)\n",
    "    # Generate sine wave\n",
    "    dti = generate_sine_wave(dti, amplitude=amplitude_scaling,\n",
    "                                    vertical_shift=vertical_shift)\n",
    "    dti.rename('timestamp')\n",
    "    # Generate noise\n",
    "    for index, values in dti.items():\n",
    "        noise = next(noise_gen(noise_scaling))\n",
    "        dti[index] = apply_noise(values, noise)\n",
    "\n",
    "\n",
    "\n",
    "    #add event consumption\n",
    "    dtis = distribute_event_costs_series(dti, events, __BASECOSTS__)\n",
    "\n",
    "    return dtis\n",
    "\n",
    "# Run the experiment with all parameters\n",
    "\n",
    "parameters = list(gen_experiment_parameters())\n",
    "for parameter in parameters:\n",
    "    result_dfs = run_experiment(start, end, vertical_shift,parameter[0], parameter[1], parameter[2])\n",
    "    # Save the results to CSV files\n",
    "    for result, basecost in zip(result_dfs, event_base_cost_scaling_list):\n",
    "        with open(f\"experiment_{parameter[0]}_{basecost}_{parameter[1]}_{parameter[2]}.csv\", \"w\") as f:\n",
    "            result.to_csv(f, index=False)\n",
    "            print(f\"Experiment with parameters {parameter} finished.\")\n",
    "\n",
    "    #end"
   ],
   "id": "8a1d9bbc22587b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SRM: Created: 0.0% processed\n",
      "1625 to process for SRM: Created\n",
      "Processing SRM: Complete: 0.0% processed\n",
      "1625 to process for SRM: Complete\n",
      "Processing SRM: Awaiting Approval: 0.0% processed\n",
      "1625 to process for SRM: Awaiting Approval\n",
      "Processing SRM: Document Completed: 0.0% processed\n",
      "Processing SRM: In Transfer to Execution Syst.: 0.0% processed\n",
      "1687 to process for SRM: In Transfer to Execution Syst.\n",
      "Processing SRM: Ordered: 0.0% processed\n",
      "1625 to process for SRM: Ordered\n",
      "Processing SRM: Change was Transmitted: 0.0% processed\n",
      "Processing Create Purchase Order Item: 0.0% processed\n",
      "251352 to process for Create Purchase Order Item\n",
      "Processing Vendor creates invoice: 0.0% processed\n",
      "216700 to process for Vendor creates invoice\n",
      "Processing Record Goods Receipt: 0.0% processed\n",
      "305837 to process for Record Goods Receipt\n",
      "Processing Record Invoice Receipt: 0.0% processed\n",
      "218375 to process for Record Invoice Receipt\n",
      "Processing Clear Invoice: 0.0% processed\n",
      "Processing Record Service Entry Sheet: 0.0% processed\n",
      "164657 to process for Record Service Entry Sheet\n",
      "Processing Cancel Goods Receipt: 0.0% processed\n",
      "3047 to process for Cancel Goods Receipt\n",
      "Processing Vendor creates debit memo: 0.0% processed\n",
      "6213 to process for Vendor creates debit memo\n",
      "Processing Cancel Invoice Receipt: 0.0% processed\n",
      "6768 to process for Cancel Invoice Receipt\n",
      "Processing Remove Payment Block: 0.0% processed\n",
      "54216 to process for Remove Payment Block\n",
      "Processing SRM: Deleted: 0.0% processed\n",
      "185 to process for SRM: Deleted\n",
      "Processing Change Price: 0.0% processed\n",
      "12189 to process for Change Price\n",
      "Processing Delete Purchase Order Item: 0.0% processed\n",
      "Processing SRM: Transaction Completed: 0.0% processed\n",
      "8 to process for SRM: Transaction Completed\n",
      "Processing Change Quantity: 0.0% processed\n",
      "20924 to process for Change Quantity\n",
      "Processing Change Delivery Indicator: 0.0% processed\n",
      "3172 to process for Change Delivery Indicator\n",
      "Processing Change Final Invoice Indicator: 0.0% processed\n",
      "11 to process for Change Final Invoice Indicator\n",
      "Processing SRM: Incomplete: 0.0% processed\n",
      "6 to process for SRM: Incomplete\n",
      "Processing SRM: Held: 0.0% processed\n",
      "6 to process for SRM: Held\n",
      "Processing Receive Order Confirmation: 0.0% processed\n",
      "32027 to process for Receive Order Confirmation\n",
      "Processing Cancel Subsequent Invoice: 0.0% processed\n",
      "463 to process for Cancel Subsequent Invoice\n",
      "Processing Reactivate Purchase Order Item: 0.0% processed\n",
      "535 to process for Reactivate Purchase Order Item\n",
      "Processing Update Order Confirmation: 0.0% processed\n",
      "242 to process for Update Order Confirmation\n",
      "Processing Block Purchase Order Item: 0.0% processed\n",
      "486 to process for Block Purchase Order Item\n",
      "Processing Change Approval for Purchase Order: 0.0% processed\n",
      "6961 to process for Change Approval for Purchase Order\n",
      "Processing Release Purchase Order: 0.0% processed\n",
      "1610 to process for Release Purchase Order\n",
      "Processing Record Subsequent Invoice: 0.0% processed\n",
      "151 to process for Record Subsequent Invoice\n",
      "Processing Set Payment Block: 0.0% processed\n",
      "123 to process for Set Payment Block\n",
      "Processing Create Purchase Requisition Item: 0.0% processed\n",
      "46585 to process for Create Purchase Requisition Item\n",
      "Processing Change Storage Location: 0.0% processed\n",
      "403 to process for Change Storage Location\n",
      "Processing Change Currency: 0.0% processed\n",
      "35 to process for Change Currency\n",
      "Processing Change payment term: 0.0% processed\n",
      "7 to process for Change payment term\n",
      "Processing Change Rejection Indicator: 0.0% processed\n",
      "2 to process for Change Rejection Indicator\n",
      "Processing Release Purchase Requisition: 0.0% processed\n",
      "467 to process for Release Purchase Requisition\n",
      "Experiment with parameters [0.1, 1, '30min'] finished.\n",
      "Experiment with parameters [0.1, 1, '30min'] finished.\n",
      "Experiment with parameters [0.1, 1, '30min'] finished.\n",
      "Processing SRM: Created: 0.0% processed\n",
      "1625 to process for SRM: Created\n",
      "Processing SRM: Complete: 0.0% processed\n",
      "1625 to process for SRM: Complete\n",
      "Processing SRM: Awaiting Approval: 0.0% processed\n",
      "1625 to process for SRM: Awaiting Approval\n",
      "Processing SRM: Document Completed: 0.0% processed\n",
      "Processing SRM: In Transfer to Execution Syst.: 0.0% processed\n",
      "1687 to process for SRM: In Transfer to Execution Syst.\n",
      "Processing SRM: Ordered: 0.0% processed\n",
      "1625 to process for SRM: Ordered\n",
      "Processing SRM: Change was Transmitted: 0.0% processed\n",
      "Processing Create Purchase Order Item: 0.0% processed\n",
      "251352 to process for Create Purchase Order Item\n",
      "Processing Vendor creates invoice: 0.0% processed\n",
      "216700 to process for Vendor creates invoice\n",
      "Processing Record Goods Receipt: 0.0% processed\n",
      "305837 to process for Record Goods Receipt\n",
      "Processing Record Invoice Receipt: 0.0% processed\n",
      "218375 to process for Record Invoice Receipt\n",
      "Processing Clear Invoice: 0.0% processed\n",
      "Processing Record Service Entry Sheet: 0.0% processed\n",
      "164657 to process for Record Service Entry Sheet\n",
      "Processing Cancel Goods Receipt: 0.0% processed\n",
      "3047 to process for Cancel Goods Receipt\n",
      "Processing Vendor creates debit memo: 0.0% processed\n",
      "6213 to process for Vendor creates debit memo\n",
      "Processing Cancel Invoice Receipt: 0.0% processed\n",
      "6768 to process for Cancel Invoice Receipt\n",
      "Processing Remove Payment Block: 0.0% processed\n",
      "54216 to process for Remove Payment Block\n",
      "Processing SRM: Deleted: 0.0% processed\n",
      "185 to process for SRM: Deleted\n",
      "Processing Change Price: 0.0% processed\n",
      "12189 to process for Change Price\n",
      "Processing Delete Purchase Order Item: 0.0% processed\n",
      "Processing SRM: Transaction Completed: 0.0% processed\n",
      "8 to process for SRM: Transaction Completed\n",
      "Processing Change Quantity: 0.0% processed\n",
      "20924 to process for Change Quantity\n",
      "Processing Change Delivery Indicator: 0.0% processed\n",
      "3172 to process for Change Delivery Indicator\n",
      "Processing Change Final Invoice Indicator: 0.0% processed\n",
      "11 to process for Change Final Invoice Indicator\n",
      "Processing SRM: Incomplete: 0.0% processed\n",
      "6 to process for SRM: Incomplete\n",
      "Processing SRM: Held: 0.0% processed\n",
      "6 to process for SRM: Held\n",
      "Processing Receive Order Confirmation: 0.0% processed\n",
      "32027 to process for Receive Order Confirmation\n",
      "Processing Cancel Subsequent Invoice: 0.0% processed\n",
      "463 to process for Cancel Subsequent Invoice\n",
      "Processing Reactivate Purchase Order Item: 0.0% processed\n",
      "535 to process for Reactivate Purchase Order Item\n",
      "Processing Update Order Confirmation: 0.0% processed\n",
      "242 to process for Update Order Confirmation\n",
      "Processing Block Purchase Order Item: 0.0% processed\n",
      "486 to process for Block Purchase Order Item\n",
      "Processing Change Approval for Purchase Order: 0.0% processed\n",
      "6961 to process for Change Approval for Purchase Order\n",
      "Processing Release Purchase Order: 0.0% processed\n",
      "1610 to process for Release Purchase Order\n",
      "Processing Record Subsequent Invoice: 0.0% processed\n",
      "151 to process for Record Subsequent Invoice\n",
      "Processing Set Payment Block: 0.0% processed\n",
      "123 to process for Set Payment Block\n",
      "Processing Create Purchase Requisition Item: 0.0% processed\n",
      "46585 to process for Create Purchase Requisition Item\n",
      "Processing Change Storage Location: 0.0% processed\n",
      "403 to process for Change Storage Location\n",
      "Processing Change Currency: 0.0% processed\n",
      "35 to process for Change Currency\n",
      "Processing Change payment term: 0.0% processed\n",
      "7 to process for Change payment term\n",
      "Processing Change Rejection Indicator: 0.0% processed\n",
      "2 to process for Change Rejection Indicator\n",
      "Processing Release Purchase Requisition: 0.0% processed\n",
      "467 to process for Release Purchase Requisition\n",
      "Experiment with parameters [1, 1, '30min'] finished.\n",
      "Experiment with parameters [1, 1, '30min'] finished.\n",
      "Experiment with parameters [1, 1, '30min'] finished.\n",
      "Processing SRM: Created: 0.0% processed\n",
      "1625 to process for SRM: Created\n",
      "Processing SRM: Complete: 0.0% processed\n",
      "1625 to process for SRM: Complete\n",
      "Processing SRM: Awaiting Approval: 0.0% processed\n",
      "1625 to process for SRM: Awaiting Approval\n",
      "Processing SRM: Document Completed: 0.0% processed\n",
      "Processing SRM: In Transfer to Execution Syst.: 0.0% processed\n",
      "1687 to process for SRM: In Transfer to Execution Syst.\n",
      "Processing SRM: Ordered: 0.0% processed\n",
      "1625 to process for SRM: Ordered\n",
      "Processing SRM: Change was Transmitted: 0.0% processed\n",
      "Processing Create Purchase Order Item: 0.0% processed\n",
      "251352 to process for Create Purchase Order Item\n",
      "Processing Vendor creates invoice: 0.0% processed\n",
      "216700 to process for Vendor creates invoice\n",
      "Processing Record Goods Receipt: 0.0% processed\n",
      "305837 to process for Record Goods Receipt\n",
      "Processing Record Invoice Receipt: 0.0% processed\n",
      "218375 to process for Record Invoice Receipt\n",
      "Processing Clear Invoice: 0.0% processed\n",
      "Processing Record Service Entry Sheet: 0.0% processed\n",
      "164657 to process for Record Service Entry Sheet\n",
      "Processing Cancel Goods Receipt: 0.0% processed\n",
      "3047 to process for Cancel Goods Receipt\n",
      "Processing Vendor creates debit memo: 0.0% processed\n",
      "6213 to process for Vendor creates debit memo\n",
      "Processing Cancel Invoice Receipt: 0.0% processed\n",
      "6768 to process for Cancel Invoice Receipt\n",
      "Processing Remove Payment Block: 0.0% processed\n",
      "54216 to process for Remove Payment Block\n",
      "Processing SRM: Deleted: 0.0% processed\n",
      "185 to process for SRM: Deleted\n",
      "Processing Change Price: 0.0% processed\n",
      "12189 to process for Change Price\n",
      "Processing Delete Purchase Order Item: 0.0% processed\n",
      "Processing SRM: Transaction Completed: 0.0% processed\n",
      "8 to process for SRM: Transaction Completed\n",
      "Processing Change Quantity: 0.0% processed\n",
      "20924 to process for Change Quantity\n",
      "Processing Change Delivery Indicator: 0.0% processed\n",
      "3172 to process for Change Delivery Indicator\n",
      "Processing Change Final Invoice Indicator: 0.0% processed\n",
      "11 to process for Change Final Invoice Indicator\n",
      "Processing SRM: Incomplete: 0.0% processed\n",
      "6 to process for SRM: Incomplete\n",
      "Processing SRM: Held: 0.0% processed\n",
      "6 to process for SRM: Held\n",
      "Processing Receive Order Confirmation: 0.0% processed\n",
      "32027 to process for Receive Order Confirmation\n",
      "Processing Cancel Subsequent Invoice: 0.0% processed\n",
      "463 to process for Cancel Subsequent Invoice\n",
      "Processing Reactivate Purchase Order Item: 0.0% processed\n",
      "535 to process for Reactivate Purchase Order Item\n",
      "Processing Update Order Confirmation: 0.0% processed\n",
      "242 to process for Update Order Confirmation\n",
      "Processing Block Purchase Order Item: 0.0% processed\n",
      "486 to process for Block Purchase Order Item\n",
      "Processing Change Approval for Purchase Order: 0.0% processed\n",
      "6961 to process for Change Approval for Purchase Order\n",
      "Processing Release Purchase Order: 0.0% processed\n",
      "1610 to process for Release Purchase Order\n",
      "Processing Record Subsequent Invoice: 0.0% processed\n",
      "151 to process for Record Subsequent Invoice\n",
      "Processing Set Payment Block: 0.0% processed\n",
      "123 to process for Set Payment Block\n",
      "Processing Create Purchase Requisition Item: 0.0% processed\n",
      "46585 to process for Create Purchase Requisition Item\n",
      "Processing Change Storage Location: 0.0% processed\n",
      "403 to process for Change Storage Location\n",
      "Processing Change Currency: 0.0% processed\n",
      "35 to process for Change Currency\n",
      "Processing Change payment term: 0.0% processed\n",
      "7 to process for Change payment term\n",
      "Processing Change Rejection Indicator: 0.0% processed\n",
      "2 to process for Change Rejection Indicator\n",
      "Processing Release Purchase Requisition: 0.0% processed\n",
      "467 to process for Release Purchase Requisition\n",
      "Experiment with parameters [10, 1, '30min'] finished.\n",
      "Experiment with parameters [10, 1, '30min'] finished.\n",
      "Experiment with parameters [10, 1, '30min'] finished.\n",
      "Processing SRM: Created: 0.0% processed\n",
      "1625 to process for SRM: Created\n",
      "Processing SRM: Complete: 0.0% processed\n",
      "1625 to process for SRM: Complete\n",
      "Processing SRM: Awaiting Approval: 0.0% processed\n",
      "1625 to process for SRM: Awaiting Approval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 118\u001B[39m\n\u001B[32m    116\u001B[39m parameters = \u001B[38;5;28mlist\u001B[39m(gen_experiment_parameters())\n\u001B[32m    117\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m parameter \u001B[38;5;129;01min\u001B[39;00m parameters:\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m     result_dfs = \u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical_shift\u001B[49m\u001B[43m,\u001B[49m\u001B[43mparameter\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameter\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameter\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    119\u001B[39m     \u001B[38;5;66;03m# Save the results to CSV files\u001B[39;00m\n\u001B[32m    120\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m result, basecost \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(result_dfs, event_base_cost_scaling_list):\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 110\u001B[39m, in \u001B[36mrun_experiment\u001B[39m\u001B[34m(start, end, vertical_shift, noise_scaling, amplitude_scaling, sampling_rate)\u001B[39m\n\u001B[32m    105\u001B[39m     dti[index] = apply_noise(values, noise)\n\u001B[32m    109\u001B[39m \u001B[38;5;66;03m#add event consumption\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m110\u001B[39m dtis = \u001B[43mdistribute_event_costs_series\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdti\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m__BASECOSTS__\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m dtis\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 134\u001B[39m, in \u001B[36mdistribute_event_costs_series\u001B[39m\u001B[34m(energy_series, events, cost_mapping, timestamp_format)\u001B[39m\n\u001B[32m    129\u001B[39m             event_end = event_end.tz_localize(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    132\u001B[39m     \u001B[38;5;66;03m# Find all intervals that overlap with the event\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m         interval_ends = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m            \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresult_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mresult_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mresult_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m[\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mfloor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mduration\u001B[49m\u001B[43m/\u001B[49m\u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTimedelta\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseconds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterval_duration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtotal_seconds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    139\u001B[39m         \u001B[38;5;66;03m#mask = (result_df[timestamp_col] <= event_end) & (interval_ends >= event_start)\u001B[39;00m\n\u001B[32m    141\u001B[39m         overlapping_intervals = result_df[\n\u001B[32m    142\u001B[39m             (result_df[timestamp_col] <= event_end.to_datetime64()) &\n\u001B[32m    143\u001B[39m             (result_df[timestamp_col].shift(-\u001B[32m1\u001B[39m, fill_value=result_df[timestamp_col].iloc[-\u001B[32m1\u001B[39m] +\n\u001B[32m    144\u001B[39m                                      timedelta(seconds=interval_duration)) >= event_start.to_datetime64())\n\u001B[32m    145\u001B[39m         ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mX:\\PHD\\lehner_phd_overview\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:584\u001B[39m, in \u001B[36mSeries.__init__\u001B[39m\u001B[34m(self, data, index, dtype, name, copy, fastpath)\u001B[39m\n\u001B[32m    582\u001B[39m         data = data.copy()\n\u001B[32m    583\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m584\u001B[39m     data = \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    586\u001B[39m     manager = _get_option(\u001B[33m\"\u001B[39m\u001B[33mmode.data_manager\u001B[39m\u001B[33m\"\u001B[39m, silent=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    587\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m manager == \u001B[33m\"\u001B[39m\u001B[33mblock\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mX:\\PHD\\lehner_phd_overview\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:654\u001B[39m, in \u001B[36msanitize_array\u001B[39m\u001B[34m(data, index, dtype, copy, allow_2d)\u001B[39m\n\u001B[32m    651\u001B[39m     subarr = _try_cast(data, dtype, copy)\n\u001B[32m    653\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m654\u001B[39m     subarr = \u001B[43mmaybe_convert_platform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    655\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m subarr.dtype == \u001B[38;5;28mobject\u001B[39m:\n\u001B[32m    656\u001B[39m         subarr = cast(np.ndarray, subarr)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mX:\\PHD\\lehner_phd_overview\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:130\u001B[39m, in \u001B[36mmaybe_convert_platform\u001B[39m\u001B[34m(values)\u001B[39m\n\u001B[32m    127\u001B[39m arr: ArrayLike\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(values, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, \u001B[38;5;28mrange\u001B[39m)):\n\u001B[32m--> \u001B[39m\u001B[32m130\u001B[39m     arr = \u001B[43mconstruct_1d_object_array_from_listlike\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    132\u001B[39m     \u001B[38;5;66;03m# The caller is responsible for ensuring that we have np.ndarray\u001B[39;00m\n\u001B[32m    133\u001B[39m     \u001B[38;5;66;03m#  or ExtensionArray here.\u001B[39;00m\n\u001B[32m    134\u001B[39m     arr = values\n",
      "\u001B[36mFile \u001B[39m\u001B[32mX:\\PHD\\lehner_phd_overview\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1599\u001B[39m, in \u001B[36mconstruct_1d_object_array_from_listlike\u001B[39m\u001B[34m(values)\u001B[39m\n\u001B[32m   1580\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1581\u001B[39m \u001B[33;03mTransform any list-like object in a 1-dimensional numpy array of object\u001B[39;00m\n\u001B[32m   1582\u001B[39m \u001B[33;03mdtype.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1595\u001B[39m \u001B[33;03m1-dimensional numpy array of dtype object\u001B[39;00m\n\u001B[32m   1596\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1597\u001B[39m \u001B[38;5;66;03m# numpy will try to interpret nested lists as further dimensions, hence\u001B[39;00m\n\u001B[32m   1598\u001B[39m \u001B[38;5;66;03m# making a 1D array that contains list-likes is a bit tricky:\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1599\u001B[39m result = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mobject\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1600\u001B[39m result[:] = values\n\u001B[32m   1601\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To reduce the waiting time a reindex and sum of the dataframes is possible to illustrate how behavior of the data changes when the measurement interval is changed. Also the changes of the periodic signal can be converted by multiplication/addition of the non event energy values. Consider that this does conserve the random noise of the original dataframe and a generation from the method above will apply the noise again during generation.",
   "id": "e9527d727bfd06c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T16:51:15.545797Z",
     "start_time": "2025-04-21T16:51:14.998617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_30min_to_1h_reset_index(df, timestamp_column='timestamp'):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame with 30-minute interval timestamps in a column to 1-hour intervals\n",
    "    by summing pairs of consecutive rows.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with timestamps in a column at 30-minute intervals\n",
    "    timestamp_column (str): Name of the column containing timestamp values\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame with timestamps at 1-hour intervals\n",
    "    \"\"\"\n",
    "    # Create a copy of the input DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Ensure the timestamp column is datetime type\n",
    "    df_copy[timestamp_column] = pd.to_datetime(df_copy[timestamp_column])\n",
    "\n",
    "    # Set the timestamp column as the index temporarily\n",
    "    df_indexed = df_copy.set_index(timestamp_column)\n",
    "\n",
    "    # Resample to hourly intervals and sum\n",
    "    df_hourly = df_indexed.resample('1h').sum()\n",
    "\n",
    "    # Reset the index to move timestamps back to a column\n",
    "    df_hourly_reset = df_hourly.reset_index()\n",
    "\n",
    "    return df_hourly_reset\n",
    "\n",
    "def resize_sine_wave(df, sine_amplitude_scalar, timestamp_column='timestamp'):\n",
    "    \"\"\"\n",
    "    Resize a DataFrame with sine wave values to a new amplitude.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame with sine wave values\n",
    "    sine_amplitude_scalar (float): Scalar to resize the sine wave amplitude\n",
    "    timestamp_column (str): Name of the column containing timestamp values\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Resized DataFrame with new sine amplitude\n",
    "    \"\"\"\n",
    "    # Ensure the timestamp column is datetime type\n",
    "    df[timestamp_column] = pd.to_datetime(df[timestamp_column])\n",
    "\n",
    "    # adjust total energy consumption by adding the sine wave energy\n",
    "    df['total'] = df['total'] + df['energy'] * (1-sine_amplitude_scalar)\n",
    "\n",
    "    # adjust energy values by multiplying with the sine amplitude scalar\n",
    "    df['energy'] = df['energy'] * sine_amplitude_scalar\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "resample_experiments = []\n",
    "with open('experiment_1_0.1_1_30min.csv', 'r') as f:\n",
    "    resample_experiments.append(pd.read_csv(f))\n",
    "with open('experiment_1_1_1_30min.csv', 'r') as f:\n",
    "    resample_experiments.append(pd.read_csv(f))\n",
    "with open('experiment_1_10_1_30min.csv', 'r') as f:\n",
    "    resample_experiments.append(pd.read_csv(f))\n",
    "\n",
    "for dataframe, basecost_name in zip(resample_experiments, [0.1, 1, 10]):\n",
    "    dataframe = convert_30min_to_1h_reset_index(dataframe)\n",
    "    with open(f\"resample_experiment_1_{basecost_name}_1_to1h.csv\", \"w\") as f:\n",
    "        dataframe.to_csv(f, index=False)\n",
    "        print(f\"Resize for {basecost_name} finished.\")\n",
    "\n",
    "scale_amplitude_experiments = []\n",
    "with open('experiment_1_0.1_1_30min.csv', 'r') as f:\n",
    "    scale_amplitude_experiments.append(pd.read_csv(f))\n",
    "with open('experiment_1_1_1_30min.csv', 'r') as f:\n",
    "    scale_amplitude_experiments.append(pd.read_csv(f))\n",
    "with open('experiment_1_10_1_30min.csv', 'r') as f:\n",
    "    scale_amplitude_experiments.append(pd.read_csv(f))\n",
    "\n",
    "for scale_dataframe, basecost_name in zip(scale_amplitude_experiments, [0.1, 1, 10]):\n",
    "    scale_dataframe = resize_sine_wave(scale_dataframe, 1.5)\n",
    "    with open(f\"rescale_experiment_1_{basecost_name}_to1.5_30min.csv\", \"w\") as f:\n",
    "        scale_dataframe.to_csv(f, index=False)\n",
    "        print(f\"Rescale for {basecost_name} finished.\")"
   ],
   "id": "aebd91fab0a5379f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resize for 0.1 finished.\n",
      "Resize for 1 finished.\n",
      "Resize for 10 finished.\n",
      "Resize for 0.1 finished.\n",
      "Resize for 1 finished.\n",
      "Resize for 10 finished.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
